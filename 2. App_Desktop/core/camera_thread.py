import cv2
from PySide6.QtCore import QThread, Signal, Qt
from PySide6.QtGui import QImage
import time
import os
import sys

# --- C·∫§U H√åNH DEBUG ---
# ƒê·∫∑t th√†nh FALSE ƒë·ªÉ ch·∫°y ch·∫ø ƒë·ªô camera th·ª±c
STATIC_IMAGE_DEBUG = False 
STATIC_IMAGE_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'resources', 'images', 'test_plate.jpg'))

# Import config
try:
    sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
    from config import AI_SKIP_FRAMES, AI_MIN_CONFIDENCE
except ImportError:
    AI_SKIP_FRAMES = 5
    AI_MIN_CONFIDENCE = 2

# --- ƒêi·ªÅu ch·ªânh sys.path ƒë·ªÉ truy c·∫≠p 1. AI_Module ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
AI_MODULE_PATH = os.path.abspath(os.path.join(CURRENT_DIR, '..', '..', '1. AI_Module'))
if AI_MODULE_PATH not in sys.path:
    sys.path.append(AI_MODULE_PATH)

try:
    from LPR_Processor_PaddleOCR import LPR_Processor 
except ImportError:
    try:
        from LPR_Processor_v2 import LPR_Processor
    except ImportError:
        try:
            from LPR_Processor import LPR_Processor
        except ImportError:
            print("FATAL ERROR: Khong the tim thay LPR_Processor.py trong 1. AI_Module/")
            class LPR_Processor:
                def __init__(self): pass
                def recognize(self, frame): return frame, "L·ªñI LPR MODULE"
# ----------------------------------------------------


class CameraThread(QThread):
    change_pixmap_signal = Signal(QImage)
    lpr_result_signal = Signal(str)
    capture_complete_signal = Signal(QImage, str)  # (captured_image, plate_text)

    WIDTH = 480
    HEIGHT = 640

    def __init__(self, camera_id, enable_ai=True):
        global STATIC_IMAGE_DEBUG
        
        super().__init__()
        self.camera_id = camera_id
        self._run_flag = True
        self.enable_ai = enable_ai
        self.capture_requested = False
        self.cap = None
        self.current_frame = None
        
        # Kh·ªüi t·∫°o AI system
        print(f"[CAMERA {camera_id}] ƒêang kh·ªüi t·∫°o AI system...")
        self.lpr_system = LPR_Processor()
        print(f"[CAMERA {camera_id}] AI system ƒë√£ s·∫µn s√†ng!")

        if STATIC_IMAGE_DEBUG and not os.path.exists(STATIC_IMAGE_PATH):
            print(f"L·ªñI DEBUG: Khong tim thay file anh tƒ©nh t·∫°i {STATIC_IMAGE_PATH}. Chay lai ch·∫ø ƒë·ªô Camera.")
            STATIC_IMAGE_DEBUG = False

    def _convert_cv_qt(self, cv_img):
        """Chuy·ªÉn ƒë·ªïi ·∫£nh OpenCV sang QImage ƒë·ªÉ hi·ªÉn th·ªã"""
        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_image.shape
        bytes_per_line = ch * w
        convert_to_Qt_format = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
        return convert_to_Qt_format.scaled(self.WIDTH, self.HEIGHT, Qt.KeepAspectRatio)


    def run(self):
        if STATIC_IMAGE_DEBUG:
            # --- CH·∫æ ƒê·ªò DEBUG ·∫¢NH Tƒ®NH ---
            processed_img, recognized_plate = self.lpr_system.recognize_from_file(STATIC_IMAGE_PATH)
            
            if recognized_plate.startswith("L·ªñI"):
                self.lpr_result_signal.emit(recognized_plate)
                return

            qt_img = self._convert_cv_qt(processed_img)

            while self._run_flag:
                # G·ª≠i t√≠n hi·ªáu li√™n t·ª•c ƒë·ªÉ GUI c·∫≠p nh·∫≠t
                print(f"[LPR DEBUG STATIC] Detected: {recognized_plate}")
                self.lpr_result_signal.emit(recognized_plate)
                self.change_pixmap_signal.emit(qt_img)
                self.msleep(500) # G·ª≠i l·∫°i m·ªói 0.5s

        else:
            # --- CH·∫æ ƒê·ªò CAMERA SNAPSHOT (ch·ªâ hi·ªÉn th·ªã, kh√¥ng x·ª≠ l√Ω AI li√™n t·ª•c) ---
            self.cap = cv2.VideoCapture(self.camera_id, cv2.CAP_DSHOW)
            
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.WIDTH)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.HEIGHT)
            self.cap.set(cv2.CAP_PROP_FPS, 20)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            if not self.cap.isOpened():
                self.lpr_result_signal.emit(f"L·ªñI CAMERA {self.camera_id} KH√îNG HO·∫†T ƒê·ªòNG!")
                self._run_flag = False
                return

            print(f"[CAMERA {self.camera_id}] ƒê√£ k·∫øt n·ªëi - CH·∫æ ƒê·ªò SNAPSHOT (qu√©t th·∫ª ƒë·ªÉ ch·ª•p)")
            
            while self._run_flag:
                ret, cv_img = self.cap.read()
                if ret:
                    self.current_frame = cv_img.copy()
                    
                    # X·ª≠ l√Ω n·∫øu c√≥ y√™u c·∫ßu ch·ª•p ·∫£nh
                    if self.capture_requested:
                        self.capture_requested = False
                        print(f"[CAMERA {self.camera_id}] üì∏ ƒêang ch·ª•p v√† nh·∫≠n di·ªán...")
                        
                        try:
                            # Ch·ª•p ·∫£nh v√† nh·∫≠n di·ªán b·∫±ng AI
                            processed_img, license_plate = self.lpr_system.recognize(self.current_frame)
                            
                            # Chuy·ªÉn sang QImage ƒë·ªÉ hi·ªÉn th·ªã
                            qt_img = self._convert_cv_qt(processed_img)
                            
                            print(f"[CAMERA {self.camera_id}] ‚úÖ Nh·∫≠n di·ªán: {license_plate}")
                            
                            # G·ª≠i t√≠n hi·ªáu k√®m ·∫£nh v√† bi·ªÉn s·ªë
                            self.capture_complete_signal.emit(qt_img, license_plate)
                            self.lpr_result_signal.emit(license_plate)
                            
                        except Exception as e:
                            print(f"[CAMERA {self.camera_id}] L·ªói AI: {e}")
                            self.lpr_result_signal.emit("L·ªñI NH·∫¨N DI·ªÜN")
                    
                    # Hi·ªÉn th·ªã live preview (kh√¥ng x·ª≠ l√Ω AI)
                    display_frame = self.current_frame.copy()
                    status_text = f"CAM {self.camera_id} | S·∫µn s√†ng - Qu√©t th·∫ª ƒë·ªÉ ch·ª•p"
                    cv2.putText(display_frame, status_text, (10, 25), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                    
                    qt_img = self._convert_cv_qt(display_frame)
                    self.change_pixmap_signal.emit(qt_img)
                    
                    self.msleep(30)
                else:
                    self.msleep(50)
            
            self.cap.release()
            print(f"[CAMERA {self.camera_id}] ƒê√£ ng·∫Øt k·∫øt n·ªëi")

    def trigger_capture(self):
        """Y√™u c·∫ßu camera ch·ª•p ·∫£nh v√† nh·∫≠n di·ªán (g·ªçi khi qu√©t th·∫ª RFID)"""
        print(f"[CAMERA {self.camera_id}] trigger_capture() ƒë∆∞·ª£c g·ªçi!")
        print(f"[CAMERA {self.camera_id}] _run_flag = {self._run_flag}")
        if self._run_flag:
            self.capture_requested = True
            print(f"[CAMERA {self.camera_id}] ‚úÖ capture_requested = True")
        else:
            print(f"[CAMERA {self.camera_id}] ‚ùå Camera ch∆∞a ch·∫°y (_run_flag = False)")
    
    def stop(self):
        self._run_flag = False
        if self.cap:
            self.cap.release()
        self.wait()